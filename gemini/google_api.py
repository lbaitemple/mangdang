#
# Copyright 2024 MangDang (www.mangdang.net)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Description: This Python script is designed to handle various AI-driven tasks, including text-based conversations,
# speech recognition (speech-to-text), and speech synthesis (text-to-speech). It integrates Google Cloud services and other libraries to accomplish these tasks.
#
# Gemini Test Method: type 'text' followed by a ' ' (space), and the text you want to type, then press enter.
# Gemini Visio Pro Test method: type 'image' followed by a ' ' (space), and the text you want to type, then press enter.
# Speech-T-Text Test method: type 'text'. After pressing enter, start speaking, then press enter.
# Text-To-Speech Test method: type 'text' followed by a ' ' (space), and the text you want to type, then press enter.
#
# References: https://python.langchain.com/v0.1/docs/integrations/llms/google_vertex_ai_palm/
#             https://cloud.google.com/speech-to-text/docs
#             https://cloud.google.com/text-to-speech/docs
#

import logging
import os
import base64
import time
import numpy as np
import google.auth
from PIL import Image
from vertexai.preview.generative_models import Image as VertexImage
from langchain_google_vertexai import ChatVertexAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.memory import ConversationBufferMemory
from langchain.chains.conversation.base import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
)
import pyaudio
import sounddevice as sd
import soundfile as sf
from google.cloud import speech
from google.cloud import texttospeech
from io import BytesIO
import asyncio


def init_credentials(key_json_path):
    """
    Initializes Google Cloud credentials by setting the environment variable.

    Parameters:
    - key_json_path (str): The file path to the Google Cloud credentials JSON file.

    Returns:
    - credentials: The credentials object for Google Cloud authentication.
    - project_id: The project ID associated with the provided credentials.
    """
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key_json_path
    credentials, project_id = google.auth.default()
    return credentials, project_id

def create_conversation():
    """
    Creates an instance of ConversationChain for AI interactions.

    Returns:
    - conversation (ConversationChain): The conversation object initialized with the AI model and prompt template.
    """
    model = ChatVertexAI(
        model_name='gemini-1.5-pro',
        convert_system_message_to_human=True,
    )

    prompt = ChatPromptTemplate(
        messages=[
            SystemMessagePromptTemplate.from_template(
                """
                You are a small female robo puppy, your name is Puppy. You will be a helpful AI assistant.
                Your LLM api is connected to STT and several TTS models so you are able to hear the user
                and change your voice and accents whenever asked.
                After being asked to change voice, the TTS handles the process, so ALWAYS assume the voice has changed, so asnwer appropriately.
                ---
                ONLY use text and avoid any other kinds of characters from generating.
                ONLY give one breathe response.
                """
            ),
            MessagesPlaceholder(variable_name="history"),
            HumanMessagePromptTemplate.from_template("{input}"),
        ]
    )

    memory = ConversationBufferMemory(memory_key="history", return_messages=True)
    conversation = ConversationChain(llm=model, prompt=prompt, verbose=False, memory=memory)
    logging.debug("conversation create end!")
    return conversation


def ai_image_response(llm, image, text):
    """
    Generates a response from the AI model based on the provided image and text.

    Parameters:
    - llm (ChatVertexAI): The AI model instance for processing images.
    - image (PIL.Image): The image object to be processed.
    - text (str): The accompanying text for the image.

    Returns:
    - result (str): The text response generated by the AI model.
    """
    logging.debug("ai_image_response start!")
    ms_start = int(time.time() * 1000)

    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    image_bytes = buffered.getvalue()

    image_base64 = base64.b64encode(image_bytes).decode('utf-8')
    image_data_url = f"data:image/jpeg;base64,{image_base64}"

    image_message = {
        "type": "image_url",
        "image_url": {
            "url": image_data_url
        }
    }
    text_message = {"type": "text", "text": text}

    message = HumanMessage(content=[text_message, image_message])

    output = llm.invoke([message])

    #logging.debug(f"ai_image_response response: {output}")
    result = output.content
    logging.debug(f"text response: {result}")
    ms_end = int(time.time() * 1000)
    logging.debug(f"ai_image_response end, delay = {ms_end - ms_start}ms")
    return result

def main():
    logging.basicConfig(
        format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(funcName)s:%(lineno)d] - %(message)s',
        level=logging.DEBUG
    )

    current_file_path = os.path.abspath(__file__)
    os.chdir(os.path.dirname(current_file_path))
    logging.debug(f"init chdir: {os.path.dirname(current_file_path)}")

    from dotenv import load_dotenv
    load_dotenv(dotenv_path='../.env')
    api_path = os.environ.get('API_KEY_PATH', '')
    if os.path.exists(api_path):
        init_credentials(api_path)

    py_audio = init_pyaudio()
    speech_client = init_speech_to_text()
    tts_client, voice, audio_config = init_text_to_speech()
    conversation = create_conversation()
    multi_model = ChatVertexAI(model="gemini-pro-vision")

    while True:
        user_input = input("Enter function apis -- 'text'/'image'/'stt'/'tts' or 'exit' to quit: ").strip().lower()
        if not user_input:
            continue
        inputs = user_input.split()
        first_word = inputs[0]

        if "exit" == first_word:
            logging.debug("Exit!")
            break

        elif "text" == first_word:
            input_text = ' '.join(inputs[1:])
            if not input_text:
                logging.debug("No input text!")
            else:
                logging.debug(f"input text: {input_text}")
                response = ai_text_response(conversation=conversation, input_text=input_text)
                print(response)

        elif "image" == first_word:
            import media_api
            image = media_api.take_photo()

            if image is None:
                logging.debug("No image captured!")
            else:
                text_prompt = ' '.join(inputs[1:])
                if not text_prompt:
                    logging.debug("No text prompt provided!")
                else:
                    logging.debug(f"text prompt: {text_prompt}")
                    response = ai_image_response(multi_model, image=image, text=text_prompt)
        elif "stt" == first_word:
            input_text, stream = start_stt(speech_client, py_audio)

            if not input_text:
                logging.debug("No speech detected!")
            else:
                logging.debug(f"input text: {input_text}")
            time.sleep(1)
            stop_stt(stream)

        elif "tts" == first_word:
            input_text = ' '.join(inputs[1:])
            if not input_text:
                logging.debug("No input text provided!")
            else:
                logging.debug(f"TTS is speaking: {input_text}")
                text_to_speech(input_text, tts_client, voice, audio_config)

if __name__ == '__main__':
    main()
